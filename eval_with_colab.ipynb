{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fcf3cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "DATASET_NAME = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "DEVICE = \"cuda\"\n",
    "SEQ_LEN = 512\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e8851b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikitext(tokenizer, ratio=0.5):\n",
    "    dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
    "    text_texts = dataset['test']['text']\n",
    "    full_text = \"\\n\\n\".join(text_texts[:int(len(text_texts)*ratio)])\n",
    "    \n",
    "    enc = tokenizer(full_text, return_tensors='pt')\n",
    "    return enc[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c64e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(ids, seq_len):\n",
    "    n_tokens = ids.size(0)\n",
    "    n_chunks = n_tokens // seq_len\n",
    "    trunc_size = n_chunks * seq_len\n",
    "    ids = ids[:trunc_size]\n",
    "    inputs = ids.view(n_chunks, seq_len)\n",
    "    labels = inputs.clone()\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e365304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl(model, inputs, labels, device, batch_size):\n",
    "    ds = TensorDataset(inputs, labels)\n",
    "    dl = DataLoader(ds, batch_size=batch_size)\n",
    "    \n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in tqdm(dl, total=len(dl), desc=\"batches\", leave=False):\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            out = model(input_ids=batch_inputs, labels=batch_labels)\n",
    "            loss = out.loss\n",
    "            n_tokens_batch = batch_labels.numel()\n",
    "            \n",
    "            total_nll += loss.item() * n_tokens_batch\n",
    "            total_tokens += n_tokens_batch\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    elapsed = t1 - t0\n",
    "    ppl = math.exp(total_nll / total_tokens)\n",
    "    return ppl, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "278b88e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wikitext\n",
      "Tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 287644\n",
      "Creating chunks\n",
      "Computing perplexity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c65c4564f2c4bdbbfa276318a14f46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 35.48\n",
      "Elapsed time: 5.08 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading wikitext\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "print(\"Tokenizing dataset\")\n",
    "ids = load_wikitext(tokenizer, ratio=1.0)\n",
    "print(f\"Total tokens: {ids.size(0)}\")\n",
    "\n",
    "print(\"Creating chunks\")\n",
    "inputs, labels = make_chunks(ids, SEQ_LEN)\n",
    "\n",
    "print(\"Computing perplexity\")\n",
    "ppl, elapsed = compute_ppl(model, inputs, labels, DEVICE, BATCH_SIZE)\n",
    "\n",
    "print(f\"Perplexity: {ppl:.2f}\")\n",
    "print(f\"Elapsed time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4e18126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ar_ppl(model, inputs, labels, device, batch_size):\n",
    "    ds = TensorDataset(inputs, labels)\n",
    "    dl = DataLoader(ds, batch_size=batch_size)\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "    latencies = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd7fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d50bbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"dtype\": \"float32\",\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.57.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb816bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
