{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f93b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum[quanto] in /usr/local/lib/python3.12/dist-packages (2.0.0)\n",
      "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.12/dist-packages (from optimum[quanto]) (4.57.2)\n",
      "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.12/dist-packages (from optimum[quanto]) (2.9.0+cu126)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from optimum[quanto]) (25.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optimum[quanto]) (2.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from optimum[quanto]) (0.36.0)\n",
      "Requirement already satisfied: optimum-quanto>=0.2.4 in /usr/local/lib/python3.12/dist-packages (from optimum[quanto]) (0.2.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.8.0->optimum[quanto]) (1.2.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from optimum-quanto>=0.2.4->optimum[quanto]) (1.13.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from optimum-quanto>=0.2.4->optimum[quanto]) (0.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11->optimum[quanto]) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[quanto]) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.29->optimum[quanto]) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11->optimum[quanto]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11->optimum[quanto]) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[quanto]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[quanto]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[quanto]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.8.0->optimum[quanto]) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, QuantizedCache\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "!pip install \"optimum[quanto]\"\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "DATASET_NAME = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "DEVICE = \"cuda\"\n",
    "SEQ_LEN = 1024\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da87c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikitext(tokenizer, ratio=0.5):\n",
    "    dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
    "    text_texts = dataset['test']['text']\n",
    "    full_text = \"\\n\\n\".join(text_texts[:int(len(text_texts)*ratio)])\n",
    "    \n",
    "    enc = tokenizer(full_text, return_tensors='pt')\n",
    "    return enc[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f20c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows(ids, stride):\n",
    "    trunc_size = ids.size(0) - (ids.size(0) % SEQ_LEN)\n",
    "    ids = ids[:trunc_size]\n",
    "    windows = []\n",
    "    for i in range(0, ids.size(0) - SEQ_LEN + 1, stride):\n",
    "        windows.append(ids[i:i+SEQ_LEN])\n",
    "    return torch.stack(windows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e444948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl_cache(model, windows, prefill_len, scored_len, forward_kwargs, window_batch_size=16):\n",
    "    B, L = windows.shape\n",
    "    latencies = []\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "    num_batches = (B + window_batch_size - 1) // window_batch_size\n",
    "    \n",
    "    batch = 0\n",
    "    for start in range(0, B, window_batch_size):\n",
    "        print(f\"Processing batch {batch+1}/{num_batches}\")\n",
    "        batch += 1\n",
    "        \n",
    "        end = min(start + window_batch_size, B)\n",
    "        win_batch = windows[start:end].to(DEVICE)\n",
    "        b = win_batch.size(0)\n",
    "        \n",
    "        prefill = win_batch[:, :prefill_len]\n",
    "        \n",
    "        out = model(\n",
    "            input_ids=prefill,\n",
    "            use_cache=True,\n",
    "            **forward_kwargs\n",
    "        )\n",
    "        pkvs = out.past_key_values\n",
    "    \n",
    "        for offset in range(scored_len):\n",
    "            tok = prefill_len + offset\n",
    "            \n",
    "            input_token = win_batch[:, tok-1:tok]\n",
    "            target = win_batch[:, tok]\n",
    "            t0 = time.perf_counter()\n",
    "            out = model(\n",
    "                input_ids=input_token,\n",
    "                past_key_values=pkvs,\n",
    "                use_cache=True,\n",
    "                **forward_kwargs\n",
    "            )\n",
    "            t1 = time.perf_counter()\n",
    "            pkvs = out.past_key_values\n",
    "            \n",
    "            logits = out.logits[:, -1, :]\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            nll_step = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            total_nll += nll_step.sum().item()\n",
    "            total_tokens += b\n",
    "            latencies.append(t1 - t0)\n",
    "        \n",
    "    return total_nll, total_tokens, latencies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24745f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl_no_cache(model, windows, prefill_len, scored_len, forward_kwargs=None, window_batch_size=16):\n",
    "    B, L = windows.shape\n",
    "    latencies = []\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "    num_batches = (B + window_batch_size - 1) // window_batch_size\n",
    "    \n",
    "    batch = 0\n",
    "    for start in range(0, B, window_batch_size):\n",
    "        print(f\"Processing batch {batch+1}/{num_batches}\")\n",
    "        batch += 1\n",
    "        \n",
    "        end = min(start + window_batch_size, B)\n",
    "        win_batch = windows[start:end].to(DEVICE)\n",
    "        b = win_batch.size(0)\n",
    "        context = win_batch[:, :prefill_len]\n",
    "        \n",
    "        for offset in range(scored_len):\n",
    "            tok = prefill_len + offset\n",
    "            input_context = win_batch[:, :tok]\n",
    "            target = win_batch[:, tok]\n",
    "            \n",
    "            t0 = time.perf_counter()\n",
    "            out = model(\n",
    "                input_ids=input_context,\n",
    "                use_cache=False,\n",
    "                **forward_kwargs\n",
    "            )\n",
    "            t1 = time.perf_counter()\n",
    "            \n",
    "            logits = out.logits[:, -1, :]\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            nll_step = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            total_nll += nll_step.sum().item()\n",
    "            total_tokens += b\n",
    "            latencies.append(t1 - t0)\n",
    "            \n",
    "    return total_nll, total_tokens, latencies\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665d95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl_quant_cache(model, windows, prefill_len, scored_len, forward_kwargs, window_batch_size=16, nbits=2):\n",
    "    B, L = windows.shape\n",
    "    latencies = []\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "    num_batches = (B + window_batch_size - 1) // window_batch_size\n",
    "    \n",
    "    batch = 0\n",
    "    for start in range(0, B, window_batch_size):\n",
    "        print(f\"Processing batch {batch+1}/{num_batches}\")\n",
    "        batch += 1\n",
    "        \n",
    "        end = min(start + window_batch_size, B)\n",
    "        win_batch = windows[start:end].to(DEVICE)\n",
    "        b = win_batch.size(0)\n",
    "        \n",
    "        prefill = win_batch[:, :prefill_len]\n",
    "        \n",
    "        cache_config = {\n",
    "            \"backend\": \"quanto\",\n",
    "            \"nbits\": nbits\n",
    "        }\n",
    "        \n",
    "        pkvs = QuantizedCache(config=model.config, **cache_config)\n",
    "        \n",
    "        out = model(\n",
    "            input_ids=prefill,\n",
    "            past_key_values=pkvs,\n",
    "            use_cache=True,\n",
    "            **forward_kwargs\n",
    "        )\n",
    "        pkvs = out.past_key_values\n",
    "        # print(f\"Cache Type: {type(pkvs)}\")\n",
    "    \n",
    "        for offset in range(scored_len):\n",
    "            tok = prefill_len + offset\n",
    "            \n",
    "            input_token = win_batch[:, tok-1:tok]\n",
    "            target = win_batch[:, tok]\n",
    "            t0 = time.perf_counter()\n",
    "            out = model(\n",
    "                input_ids=input_token,\n",
    "                past_key_values=pkvs,\n",
    "                use_cache=True,\n",
    "                **forward_kwargs\n",
    "            )\n",
    "            t1 = time.perf_counter()\n",
    "            pkvs = out.past_key_values\n",
    "            \n",
    "            logits = out.logits[:, -1, :]\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            nll_step = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            total_nll += nll_step.sum().item()\n",
    "            total_tokens += b\n",
    "            latencies.append(t1 - t0)\n",
    "        \n",
    "    return total_nll, total_tokens, latencies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9643cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl(model, windows, device, prefill_len, use_cache=\"normal\", forward_kwargs=None, quant_bits=2):\n",
    "    if forward_kwargs is None:\n",
    "        forward_kwargs = {}\n",
    "    model.eval()\n",
    "    windows = windows.to(device)\n",
    "    B, L = windows.shape\n",
    "    scored_len = L - prefill_len\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        if use_cache == \"normal\":\n",
    "            total_nll, total_tokens, latencies = compute_ppl_cache(\n",
    "                model, windows, prefill_len, scored_len, forward_kwargs\n",
    "            )\n",
    "        elif use_cache == \"quantized\":\n",
    "            total_nll, total_tokens, latencies = compute_ppl_quant_cache(\n",
    "                model, windows, prefill_len, scored_len, forward_kwargs, nbits=quant_bits\n",
    "            )\n",
    "        else:\n",
    "            total_nll, total_tokens, latencies = compute_ppl_no_cache(\n",
    "                model, windows, prefill_len, scored_len, forward_kwargs\n",
    "            )\n",
    "    avg_nll = total_nll / total_tokens\n",
    "    ppl = math.exp(avg_nll)\n",
    "    avg_latency_ms = 1000.0 * sum(latencies) / len(latencies)\n",
    "    return ppl, avg_latency_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc08692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ppl(model, windows, device, prefill_len, use_cache=\"normal\", forward_kwargs=None, quant_bits=2):\n",
    "    return compute_ppl(model, windows, device, prefill_len, use_cache, forward_kwargs, quant_bits=quant_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be74af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_memory(fn, *args, **kwargs):\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_alloc = torch.cuda.memory_allocated()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    result = fn(*args, **kwargs)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    peak_alloc = torch.cuda.max_memory_allocated()\n",
    "    peak_reserved = torch.cuda.max_memory_reserved()\n",
    "    \n",
    "    stats = {\n",
    "        \"start_alloc_bytes\": start_alloc,\n",
    "        \"peak_alloc_bytes\": peak_alloc,\n",
    "        \"peak_reserved_bytes\": peak_reserved,\n",
    "        \"delta_alloc_bytes\": peak_alloc - start_alloc,\n",
    "        \"elapsed_sec\": t1 - t0,\n",
    "    }\n",
    "    \n",
    "    return result, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b98f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_gpu(model, windows, device, prefill_len, forward_kwargs=None):\n",
    "    if forward_kwargs is None:\n",
    "        forward_kwargs = {}\n",
    "\n",
    "    model.eval()\n",
    "    # use a tiny subset â€“ just enough to trigger kernels + quant machinery\n",
    "    warmup_windows = windows[:2].to(device)\n",
    "\n",
    "    # modes you actually benchmark; include quant settings you care about\n",
    "    modes = [\n",
    "        {\"use_cache\": \"normal\"},\n",
    "        {\"use_cache\": \"quantized\", \"quant_bits\": 4},\n",
    "        {\"use_cache\": \"quantized\", \"quant_bits\": 2},\n",
    "        {\"use_cache\": \"no_cache\"},\n",
    "    ]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for cfg in modes:\n",
    "            print(f\"Warming up mode: {cfg}\")\n",
    "            _ = compute_ppl(\n",
    "                model,\n",
    "                warmup_windows,\n",
    "                device,\n",
    "                prefill_len=prefill_len,\n",
    "                use_cache=cfg[\"use_cache\"],\n",
    "                forward_kwargs=forward_kwargs,\n",
    "                quant_bits=cfg.get(\"quant_bits\", 2),\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf4b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wikitext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (142657 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 142657\n",
      "Creating windows\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading wikitext\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=torch.float16).to(DEVICE)\n",
    "\n",
    "print(\"Tokenizing dataset\")\n",
    "ids = load_wikitext(tokenizer, ratio=0.5)\n",
    "print(f\"Total tokens: {ids.size(0)}\")\n",
    "\n",
    "print(\"Creating windows\")\n",
    "windows = make_windows(ids, stride=SEQ_LEN//2)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d3d92f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up mode: {'use_cache': 'normal'}\n",
      "Processing batch 1/1\n",
      "Warming up mode: {'use_cache': 'quantized', 'quant_bits': 4}\n",
      "Processing batch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple distributions found for package optimum. Picked distribution: optimum-quanto\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up mode: {'use_cache': 'quantized', 'quant_bits': 2}\n",
      "Processing batch 1/1\n",
      "Warming up mode: {'use_cache': 'no_cache'}\n",
      "Processing batch 1/1\n",
      "Processing batch 1/18\n",
      "Processing batch 2/18\n",
      "Processing batch 3/18\n",
      "Processing batch 4/18\n",
      "Processing batch 5/18\n",
      "Processing batch 6/18\n",
      "Processing batch 7/18\n",
      "Processing batch 8/18\n",
      "Processing batch 9/18\n",
      "Processing batch 10/18\n",
      "Processing batch 11/18\n",
      "Processing batch 12/18\n",
      "Processing batch 13/18\n",
      "Processing batch 14/18\n",
      "Processing batch 15/18\n",
      "Processing batch 16/18\n",
      "Processing batch 17/18\n",
      "Processing batch 18/18\n",
      "Processing batch 1/18\n",
      "Processing batch 2/18\n",
      "Processing batch 3/18\n",
      "Processing batch 4/18\n",
      "Processing batch 5/18\n",
      "Processing batch 6/18\n",
      "Processing batch 7/18\n",
      "Processing batch 8/18\n",
      "Processing batch 9/18\n",
      "Processing batch 10/18\n",
      "Processing batch 11/18\n",
      "Processing batch 12/18\n",
      "Processing batch 13/18\n",
      "Processing batch 14/18\n",
      "Processing batch 15/18\n",
      "Processing batch 16/18\n",
      "Processing batch 17/18\n",
      "Processing batch 18/18\n",
      "Processing batch 1/18\n",
      "Processing batch 2/18\n",
      "Processing batch 3/18\n",
      "Processing batch 4/18\n",
      "Processing batch 5/18\n",
      "Processing batch 6/18\n",
      "Processing batch 7/18\n",
      "Processing batch 8/18\n",
      "Processing batch 9/18\n",
      "Processing batch 10/18\n",
      "Processing batch 11/18\n",
      "Processing batch 12/18\n",
      "Processing batch 13/18\n",
      "Processing batch 14/18\n",
      "Processing batch 15/18\n",
      "Processing batch 16/18\n",
      "Processing batch 17/18\n",
      "Processing batch 18/18\n",
      "Processing batch 1/18\n",
      "Processing batch 2/18\n",
      "Processing batch 3/18\n",
      "Processing batch 4/18\n",
      "Processing batch 5/18\n",
      "Processing batch 6/18\n",
      "Processing batch 7/18\n",
      "Processing batch 8/18\n",
      "Processing batch 9/18\n",
      "Processing batch 10/18\n",
      "Processing batch 11/18\n",
      "Processing batch 12/18\n",
      "Processing batch 13/18\n",
      "Processing batch 14/18\n",
      "Processing batch 15/18\n",
      "Processing batch 16/18\n",
      "Processing batch 17/18\n",
      "Processing batch 18/18\n"
     ]
    }
   ],
   "source": [
    "warmup_gpu(model, windows, DEVICE, prefill_len=SEQ_LEN // 2)\n",
    "\n",
    "(result_noCache, mem_noCache) = track_memory(\n",
    "    run_ppl,\n",
    "    model,\n",
    "    windows,\n",
    "    DEVICE,\n",
    "    SEQ_LEN // 2,\n",
    "    False,\n",
    ")\n",
    "\n",
    "ppl_noCache, avg_latency_noCache = result_noCache\n",
    "\n",
    "(result_normal, mem_normal) = track_memory(\n",
    "    run_ppl,\n",
    "    model,\n",
    "    windows,\n",
    "    DEVICE,\n",
    "    SEQ_LEN // 2,\n",
    "    \"normal\",\n",
    ")\n",
    "\n",
    "ppl_normal, avg_latency_normal = result_normal\n",
    "\n",
    "\n",
    "(result_quant2, mem_quant2) = track_memory(\n",
    "    run_ppl,\n",
    "    model,\n",
    "    windows,\n",
    "    DEVICE,\n",
    "    SEQ_LEN // 2,\n",
    "    \"quantized\",\n",
    "    quant_bits=2,\n",
    ")\n",
    "\n",
    "ppl_quant2, avg_latency_quant2 = result_quant2\n",
    "\n",
    "(result_quant4, mem_quant4) = track_memory(\n",
    "    run_ppl,\n",
    "    model,\n",
    "    windows,\n",
    "    DEVICE,\n",
    "    SEQ_LEN // 2,\n",
    "    \"quantized\",\n",
    "    quant_bits=4,\n",
    ")\n",
    "ppl_quant4, avg_latency_quant4 = result_quant4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ac993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size_bytes(model, include_buffers=True):\n",
    "    param_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_bytes = 0\n",
    "    if include_buffers:\n",
    "        buffer_bytes = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    return param_bytes + buffer_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "098a41c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8174d083-43cc-4a6a-a547-d656c911ff64\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Avg Latency (ms)</th>\n",
       "      <th>Peak Alloc (MB)</th>\n",
       "      <th>Total Elapsed (s)</th>\n",
       "      <th>Model Size (MB)</th>\n",
       "      <th>Cache Size (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Cache</th>\n",
       "      <td>24.5021</td>\n",
       "      <td>16.0020</td>\n",
       "      <td>3430.2461</td>\n",
       "      <td>479.9001</td>\n",
       "      <td>249.3501</td>\n",
       "      <td>3180.8960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal Cache</th>\n",
       "      <td>24.5416</td>\n",
       "      <td>7.6514</td>\n",
       "      <td>1932.2983</td>\n",
       "      <td>72.1479</td>\n",
       "      <td>249.3501</td>\n",
       "      <td>1682.9482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantized Cache (2-bit)</th>\n",
       "      <td>34.0366</td>\n",
       "      <td>12.0098</td>\n",
       "      <td>1207.8755</td>\n",
       "      <td>112.4732</td>\n",
       "      <td>249.3501</td>\n",
       "      <td>958.5254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantized Cache (4-bit)</th>\n",
       "      <td>24.6361</td>\n",
       "      <td>11.9507</td>\n",
       "      <td>1316.0190</td>\n",
       "      <td>111.8905</td>\n",
       "      <td>249.3501</td>\n",
       "      <td>1066.6689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8174d083-43cc-4a6a-a547-d656c911ff64')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8174d083-43cc-4a6a-a547-d656c911ff64 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8174d083-43cc-4a6a-a547-d656c911ff64');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                         Perplexity  Avg Latency (ms)  Peak Alloc (MB)  \\\n",
       "No Cache                    24.5021           16.0020        3430.2461   \n",
       "Normal Cache                24.5416            7.6514        1932.2983   \n",
       "Quantized Cache (2-bit)     34.0366           12.0098        1207.8755   \n",
       "Quantized Cache (4-bit)     24.6361           11.9507        1316.0190   \n",
       "\n",
       "                         Total Elapsed (s)  Model Size (MB)  Cache Size (MB)  \n",
       "No Cache                          479.9001         249.3501        3180.8960  \n",
       "Normal Cache                       72.1479         249.3501        1682.9482  \n",
       "Quantized Cache (2-bit)           112.4732         249.3501         958.5254  \n",
       "Quantized Cache (4-bit)           111.8905         249.3501        1066.6689  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities = {\n",
    "    \"No Cache\": round(ppl_noCache, 4),\n",
    "    \"Normal Cache\": round(ppl_normal, 4),\n",
    "    \"Quantized Cache (2-bit)\": round(ppl_quant2, 4),\n",
    "    \"Quantized Cache (4-bit)\": round(ppl_quant4, 4)\n",
    "}\n",
    "\n",
    "latencies = {\n",
    "    \"No Cache\": round(avg_latency_noCache, 4),\n",
    "    \"Normal Cache\": round(avg_latency_normal, 4),\n",
    "    \"Quantized Cache (2-bit)\": round(avg_latency_quant2, 4),\n",
    "    \"Quantized Cache (4-bit)\": round(avg_latency_quant4, 4)\n",
    "}\n",
    "\n",
    "peak_alloc = {\n",
    "    \"No Cache\": round(mem_noCache[\"peak_alloc_bytes\"] / (1024**2), 4),\n",
    "    \"Normal Cache\": round(mem_normal[\"peak_alloc_bytes\"] / (1024**2), 4),\n",
    "    \"Quantized Cache (2-bit)\": round(mem_quant2[\"peak_alloc_bytes\"] / (1024**2), 4),\n",
    "    \"Quantized Cache (4-bit)\": round(mem_quant4[\"peak_alloc_bytes\"] / (1024**2), 4)\n",
    "}\n",
    "\n",
    "total_elapsed = {\n",
    "    \"No Cache\": round(mem_noCache[\"elapsed_sec\"], 4),\n",
    "    \"Normal Cache\": round(mem_normal[\"elapsed_sec\"], 4),\n",
    "    \"Quantized Cache (2-bit)\": round(mem_quant2[\"elapsed_sec\"], 4),\n",
    "    \"Quantized Cache (4-bit)\": round(mem_quant4[\"elapsed_sec\"], 4)\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Perplexity\": perplexities,\n",
    "    \"Avg Latency (ms)\": latencies,\n",
    "    \"Peak Alloc (MB)\": peak_alloc,\n",
    "    \"Total Elapsed (s)\": total_elapsed\n",
    "})\n",
    "\n",
    "model_size = round(model_size_bytes(model) / (1024**2), 4)\n",
    "results_df[\"Model Size (MB)\"] = model_size\n",
    "results_df[\"Cache Size (MB)\"] = (results_df[\"Peak Alloc (MB)\"] - model_size).round(4)\n",
    "\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "038dc4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "print(\"\\a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b080e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
