{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f93b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "DATASET_NAME = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "DEVICE = \"cuda\"\n",
    "SEQ_LEN = 1024\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da87c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikitext(tokenizer, ratio=0.5):\n",
    "    dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
    "    text_texts = dataset['test']['text']\n",
    "    full_text = \"\\n\\n\".join(text_texts[:int(len(text_texts)*ratio)])\n",
    "    \n",
    "    enc = tokenizer(full_text, return_tensors='pt')\n",
    "    return enc[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f20c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows(ids, stride):\n",
    "    trunc_size = ids.size(0) - (ids.size(0) % SEQ_LEN)\n",
    "    ids = ids[:trunc_size]\n",
    "    windows = []\n",
    "    for i in range(0, ids.size(0) - SEQ_LEN + 1, stride):\n",
    "        windows.append(ids[i:i+SEQ_LEN])\n",
    "    return torch.stack(windows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e444948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl_cache(model, windows, prefill_len, scored_len, forward_kwargs, window_batch_size=8):\n",
    "    B, L = windows.shape\n",
    "    latencies = []\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "    num_batches = (B + window_batch_size - 1) // window_batch_size\n",
    "    batch = 0\n",
    "    for start in range(0, B, window_batch_size):\n",
    "        print(f\"Processing batch {batch+1}/{num_batches}\")\n",
    "        batch += 1\n",
    "        end = min(start + window_batch_size, B)\n",
    "        win_batch = windows[start:end].to(DEVICE)\n",
    "        b = win_batch.size(0)\n",
    "        prefill = win_batch[:, :prefill_len]\n",
    "        out = model(\n",
    "            input_ids=prefill,\n",
    "            use_cache=True,\n",
    "            **forward_kwargs\n",
    "        )\n",
    "        pkvs = out.past_key_values\n",
    "    \n",
    "        for offset in range(scored_len):\n",
    "            tok = prefill_len + offset\n",
    "            \n",
    "            input_token = win_batch[:, tok-1:tok]\n",
    "            target = win_batch[:, tok]\n",
    "            t0 = time.perf_counter()\n",
    "            out = model(\n",
    "                input_ids=input_token,\n",
    "                past_key_values=pkvs,\n",
    "                use_cache=True,\n",
    "                **forward_kwargs\n",
    "            )\n",
    "            t1 = time.perf_counter()\n",
    "            pkvs = out.past_key_values\n",
    "            \n",
    "            logits = out.logits[:, -1, :]\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            nll_step = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            total_nll += nll_step.sum().item()\n",
    "            total_tokens += b\n",
    "            latencies.append(t1 - t0)\n",
    "        \n",
    "    return total_nll, total_tokens, latencies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24745f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl_no_cache(model, windows, prefill_len, scored_len, forward_kwargs=None):\n",
    "    B, _ = windows.shape\n",
    "    latencies = []\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for offset in range(scored_len):\n",
    "        tok = prefill_len + offset\n",
    "        \n",
    "        context = windows[:, :tok]\n",
    "        target = windows[:, tok]\n",
    "        \n",
    "        t0 = time.perf_counter()\n",
    "        out = model(\n",
    "            input_ids=context,\n",
    "            use_cache=False,\n",
    "            **forward_kwargs\n",
    "        )\n",
    "        t1 = time.perf_counter()\n",
    "        logits = out.logits[:, -1, :]\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        nll_step = -log_probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        total_nll += nll_step.sum().item()\n",
    "        total_tokens += B\n",
    "        latencies.append(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9643cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl(model, windows, device, prefill_len, use_cache=True, forward_kwargs=None):\n",
    "    if forward_kwargs is None:\n",
    "        forward_kwargs = {}\n",
    "    model.eval()\n",
    "    windows = windows.to(device)\n",
    "    B, L = windows.shape\n",
    "    scored_len = L - prefill_len\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        if use_cache:\n",
    "            total_nll, total_tokens, latencies = compute_ppl_cache(\n",
    "                model, windows, prefill_len, scored_len, forward_kwargs\n",
    "            )\n",
    "        else:\n",
    "            total_nll, total_tokens, latencies = compute_ppl_no_cache(\n",
    "                model, windows, prefill_len, scored_len, forward_kwargs\n",
    "            )\n",
    "    avg_nll = total_nll / total_tokens\n",
    "    ppl = math.exp(avg_nll)\n",
    "    avg_latency_ms = 1000.0 * sum(latencies) / len(latencies)\n",
    "    return ppl, avg_latency_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf4b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wikitext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (90482 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 90482\n",
      "Creating windows\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading wikitext\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=torch.float16).to(DEVICE)\n",
    "\n",
    "print(\"Tokenizing dataset\")\n",
    "ids = load_wikitext(tokenizer, ratio=0.3)\n",
    "print(f\"Total tokens: {ids.size(0)}\")\n",
    "\n",
    "print(\"Creating windows\")\n",
    "windows = make_windows(ids, stride=SEQ_LEN//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3518d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/22\n",
      "Processing batch 2/22\n",
      "Processing batch 3/22\n",
      "Processing batch 4/22\n",
      "Processing batch 5/22\n",
      "Processing batch 6/22\n",
      "Processing batch 7/22\n",
      "Processing batch 8/22\n",
      "Processing batch 9/22\n",
      "Processing batch 10/22\n",
      "Processing batch 11/22\n",
      "Processing batch 12/22\n",
      "Processing batch 13/22\n",
      "Processing batch 14/22\n",
      "Processing batch 15/22\n",
      "Processing batch 16/22\n",
      "Processing batch 17/22\n",
      "Processing batch 18/22\n",
      "Processing batch 19/22\n",
      "Processing batch 20/22\n",
      "Processing batch 21/22\n",
      "Processing batch 22/22\n",
      "PPL with cache: 25.75, Avg latency per token: 8.01 ms\n"
     ]
    }
   ],
   "source": [
    "ppl, avg_latency_ms = compute_ppl(\n",
    "    model, windows, DEVICE, prefill_len=SEQ_LEN//2, use_cache=True\n",
    ")\n",
    "print(f\"PPL with cache: {ppl:.2f}, Avg latency per token: {avg_latency_ms:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580e577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
